<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-yolo-v9运行实录" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/10/21/yolo-v9%E8%BF%90%E8%A1%8C%E5%AE%9E%E5%BD%95/" class="article-date">
  <time class="dt-published" datetime="2024-10-21T13:34:14.000Z" itemprop="datePublished">2024-10-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/10/21/yolo-v9%E8%BF%90%E8%A1%8C%E5%AE%9E%E5%BD%95/">yolo v9运行实录</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1>怎样运行yolo v9</h1>
<h2 id="简介">简介</h2>
<p>yolo v9 是最新发布的yolo模型，一句话总结：比前代更好，更快，更强。</p>
<p>本文旨在用最简单的方法吧yolov9的代码跑起来，因此不涉及训练部分，仅教会大家怎么使用yolov9的官方权重进行图像检测。</p>
<h2 id="准备">准备</h2>
<p>某些网址可能无法打开，建议全程加速器环境下载</p>
<ol>
<li>git工具 <a target="_blank" rel="noopener" href="https://git-scm.com/download/win">Git - Downloading Package (git-scm.com)</a></li>
<li>miniconda 虚拟环境软件，也可以使用anaconda , 只是miniconda 更加轻量化。[Index of /anaconda/miniconda/ | 清华大学开源软件镜像站 | Tsinghua Open Source Mirror](<a target="_blank" rel="noopener" href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/">https://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/</a></li>
</ol>
<p>注意下载软件要选择合适的版本，主要是x86和arm版本软件记得区分，一般intel的cpu使用x64版本</p>
<p><img src="https://pics.zhouimage.top/pic/202406061807279.png" alt="image-20240606180754233"></p>
<h2 id="git安装-可选">git安装(可选)</h2>
<p>1.双击安装包安装</p>
<p><img src="https://pics.zhouimage.top/pic/202406061808633.png" alt="image-20240606180844600"></p>
<p>2.一直点击下一步</p>
<p><img src="https://pics.zhouimage.top/pic/202406061810837.png" alt="image-20240606181024808"></p>
<p>3.验证一下安装成功没有，windows+R键，左下角输入cmd,进入命令行</p>
<p><img src="https://pics.zhouimage.top/pic/202406061811142.png" alt="image-20240606181149124"></p>
<p>在命令行中输入<code>git -v</code>，显示出git版本号就是安装成功：</p>
<p><img src="https://pics.zhouimage.top/pic/202406061812756.png" alt="image-20240606181247738"></p>
<h2 id="miniconda安装">miniconda安装</h2>
<p>双击安装包安装，一直点击下一步，不过我们需要<strong>记住安装路径</strong>。后面需要用到。</p>
<p><img src="https://pics.zhouimage.top/pic/202406061825863.png" alt="image-20240606182526842"></p>
<p>安装可能需要一段时间：</p>
<p><img src="C:/Users/XIANR/AppData/Roaming/Typora/typora-user-images/image-20240606182633969.png" alt="image-20240606182633969"></p>
<p><img src="https://pics.zhouimage.top/pic/202406061827343.png" alt="image-20240606182703322"></p>
<p>把上图两个钩取消然后点击finish即可。</p>
<p>安装完成之后需要配置环境变量，在windows搜索栏搜索环境变量</p>
<p><img src="https://pics.zhouimage.top/pic/202406061828757.png" alt="image-20240606182844704"></p>
<p>点进去选择<img src="https://pics.zhouimage.top/pic/202406061829516.png" alt="image-20240606182927476"></p>
<p>点进去，新建3个环境变量：</p>
<p><code>C:\Users\XIANR\miniconda3</code></p>
<p><code>C:\Users\XIANR\miniconda3\Library\bin</code></p>
<p><code>C:\Users\XIANR\miniconda3\Scripts</code></p>
<p>注意，上面三个变量中的<code>C:\Users\XIANR\miniconda3</code>自行替换为你的安装路径，如，小明的安装路径在D:\miniconda3，那小明的三个环境变量就是：</p>
<p><code>D:\miniconda3</code></p>
<p><code>D:\miniconda3\Library\bin</code></p>
<p><code>D:\miniconda3\Scripts</code></p>
<p>配置完成后如下：</p>
<p><img src="https://pics.zhouimage.top/pic/202406061834993.png" alt="image-20240606183449970"></p>
<p>点击三次确定保存</p>
<p>然后windows+r打开CMD,输入<code>conda -V</code>出现Conda版本号即为安装成功</p>
<p><img src="https://pics.zhouimage.top/pic/202406061836886.png" alt="image-20240606183647866"></p>
<h2 id="新建python环境">新建python环境</h2>
<p>在cmd中 输入<code>conda create -n yolo python=3.9</code>，等待一段时间后，出现下面界面，输入y</p>
<p><img src="https://pics.zhouimage.top/pic/202406061840206.png" alt="image-20240606184033184"></p>
<p>注意：如果下载缓慢请开加速器</p>
<p>出现如下提示即创建环境成功：</p>
<p><img src="https://pics.zhouimage.top/pic/202406061841910.png" alt="image-20240606184146890"></p>
<p>然后输入 <code>conda init</code>关掉cmd再次打开，输入<code>conda activate yolo</code>，当左边出现yolo字样的时候，我们的python环境就创建成功了。</p>
<p><img src="https://pics.zhouimage.top/pic/202406061843805.png" alt="image-20240606184327785"></p>
<h2 id="yolo源码下载">yolo源码下载</h2>
<h3 id="如果你在第一步安装了git">如果你在第一步安装了git</h3>
<p>新建一个文件夹用来存放源码，在文件管理器的地址栏输入CMD,回车，会在当前建立的文件夹下打开CMD</p>
<p><img src="https://pics.zhouimage.top/pic/202406061846663.png" alt="image-20240606184605628"></p>
<p><img src="https://pics.zhouimage.top/pic/202406061847565.png" alt="image-20240606184724543"></p>
<p>输入<code>git clone https://github.com/WongKinYiu/yolov9.git</code></p>
<p><img src="https://pics.zhouimage.top/pic/202406061849810.png" alt="image-20240606184939785"></p>
<p>此时在文件夹中已经出现了yolov9的代码</p>
<p><img src="https://pics.zhouimage.top/pic/202406061850386.png" alt="image-20240606185052364"></p>
<h3 id="如果你没安装git">如果你没安装git</h3>
<p>在 <a target="_blank" rel="noopener" href="https://github.com/WongKinYiu/yolov9">https://github.com/WongKinYiu/yolov9</a> 中自行下载源码后解压即可。</p>
<h2 id="python包安装">python包安装</h2>
<p>进入源码文件夹，在地址栏输入cmd进入命令行</p>
<p><img src="https://pics.zhouimage.top/pic/202406061853115.png" alt="image-20240606185324091"></p>
<p>在命令行输入 <code>conda activate yolo</code></p>
<p><img src="https://pics.zhouimage.top/pic/202406061854300.png" alt="image-20240606185449273"></p>
<h3 id="如果你有加速器">如果你有加速器</h3>
<p>输入命令 <code>pip install -r requirements.txt</code></p>
<h3 id="如果你没加速器">如果你没加速器</h3>
<p>输入命令 <code>pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple</code></p>
<p>此时会自动下载依赖并安装，可能需要等待很长时间。</p>
<p><img src="https://pics.zhouimage.top/pic/202406061856934.png" alt="image-20240606185647908"></p>
<p>当出现successfuly 的时候，就成功了！如果你报错了，请重新来一遍，这是配环境的过程中难以避免的。</p>
<p><img src="https://pics.zhouimage.top/pic/202406061902343.png" alt="image-20240606190249283"></p>
<h2 id="下载预训练权重包">下载预训练权重包</h2>
<p>在 <a target="_blank" rel="noopener" href="https://github.com/WongKinYiu/yolov9/releases">https://github.com/WongKinYiu/yolov9/releases</a> 里下载预训练模型，随便挑一个pt为后缀的，下载完后，<a target="_blank" rel="noopener" href="http://xn--yolo-uh5fn22agl5a.pt">改名为yolo.pt</a>，放到代码目录下</p>
<p><img src="https://pics.zhouimage.top/pic/202406061911955.png" alt="image-20240606191130927"></p>
<h2 id="运行">运行</h2>
<p>恭喜你来到这一步，接下来就是激动的运行时间了。</p>
<p>我们首先可以找几张图片放在yolo的data/images文件夹中</p>
<p><img src="https://pics.zhouimage.top/pic/202406061904604.png" alt="image-20240606190420581"></p>
<p>回到yolo代码主目录，打开cmd.</p>
<p>然后在命令行中输入 <code>python detect.py</code> 等待一会，出现下图结果即运行成功</p>
<p><img src="https://pics.zhouimage.top/pic/202406061913172.png" alt="image-20240606191333147"></p>
<p>图中显示我们的结果在 <code> runs\detect\exp2</code>文件夹中，打开文件夹即可看到结果：</p>
<p><img src="https://pics.zhouimage.top/pic/202406061915147.png" alt="image-20240606191515123"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/10/21/yolo-v9%E8%BF%90%E8%A1%8C%E5%AE%9E%E5%BD%95/" data-id="cm2j2762f000270p6a0o4edi1" data-title="yolo v9运行实录" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-测试第2篇文章" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/10/21/%E6%B5%8B%E8%AF%95%E7%AC%AC2%E7%AF%87%E6%96%87%E7%AB%A0/" class="article-date">
  <time class="dt-published" datetime="2024-10-21T11:54:56.000Z" itemprop="datePublished">2024-10-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/10/21/%E6%B5%8B%E8%AF%95%E7%AC%AC2%E7%AF%87%E6%96%87%E7%AB%A0/">测试第2篇文章</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="Hi-👋-Here-is-the-homepage-of-Intelligent-Cognitive-Systems-Laboratory-iCOST-Beijing-University-of-Posts-and-Telecommunications">Hi 👋. Here is the homepage of Intelligent Cognitive Systems Laboratory (iCOST), Beijing University of Posts and Telecommunications.</h2>
<!--

**Here are some ideas to get you started:**

🙋‍♀️ A short introduction - what is your organization all about?
🌈 Contribution guidelines - how can the community get involved?
👩‍💻 Useful resources - where can the community find your docs? Is there anything else the community should know?
🍿 Fun facts - what does your team eat for breakfast?
🧙 Remember, you can do mighty things with the power of [Markdown](https://docs.github.com/github/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax)
-->
<h2 id="Table-of-Contents">Table of Contents</h2>
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#papers">Papers</a>
<ul>
<li><a href="#2d-human-pose-estimation">2D Human Pose Estimation</a></li>
<li><a href="#3d-human-pose-estimation">3D Human Pose Estimation</a></li>
<li><a href="#3d-human-motion-prediction">3D Human Motion Prediction</a></li>
<li><a href="#early-action-prediction">Early Action Prediction</a></li>
<li><a href="#skeleton-based-human-action-recognition">Skeleton-based Human Action Recognition</a></li>
<li><a href="#group-activity-recognition">Group Activity Recognition</a></li>
<li><a href="#uncertainty-aware-scene-understanding-with-point-clouds">Uncertainty-aware Scene Understanding with Point Clouds</a></li>
<li><a href="#audio-visual-learning">Audio-Visual Learning</a></li>
<li><a href="#audio-and-speech-processing">Audio and Speech Processing</a></li>
<li><a href="#medical-image-segmentation">Medical Image Segmentation</a></li>
<li><a href="#adversarial-attack">Adversarial Attack</a></li>
<li><a href="#others">Others</a></li>
</ul>
</li>
</ul>
<h2 id="Introduction">Introduction</h2>
<p>The Intelligent Cognitive Systems Laboratory (iCost) at BUPT (Beijing University of Posts and Telecommunications) is actively engaged in long-term research in multiple cutting-edge fields, including computer vision and embodied intelligence. Our research spans various sub-domains such as action recognition, human pose prediction and estimation, uncertainty research, multimodal and audio-visual learning, audio-visual event detection, medical image segmentation, 3D object detection, adversarial strategies, embodied navigation, and robot grasping.</p>
<p>Our research team has achieved substantial results, publishing numerous high-quality research papers in internationally recognized and authoritative journals such as IEEE Transactions on Image Processing (IEEE TIP), IEEE Transactions on Circuits and Systems for Video Technology (IEEE TCSVT), and top-tier conferences like AAAI.</p>
<p>In the Intelligent Cognitive Systems Laboratory, we encourage communication and collaboration among team members, fostering a rigorous and harmonious academic atmosphere. We warmly welcome scholars, researchers, and students who are passionate about artificial intelligence and related fields to join us or collaborate. We believe that everyone with a curiosity for science can find their stage here.</p>
<h2 id="Papers">Papers</h2>
<h3 id="2D-Human-Pose-Estimation">2D Human Pose Estimation</h3>
<p>[<strong>PR 2024</strong>] Kinematics Modeling Network for Video-based Human Pose Estimation [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2207.10971.pdf">paper</a>] [<a target="_blank" rel="noopener" href="https://github.com/YHDang/KIMNet">code</a>]</p>
<p>[<strong>TIP 2022</strong>] Relation-Based Associative Joint Location for Human Pose Estimation in Videos [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/9786543">paper</a>] [<a target="_blank" rel="noopener" href="https://github.com/YHDang/pose-estimation">code</a>]</p>
<p>[<strong>KBS 2024</strong>] DHRNet: A Dual-Path Hierarchical Relation Network for Multi-Person Pose Estimation [<a target="_blank" rel="noopener" href="https://github.com/YHDang/DHRNet">code</a>]</p>
<p>[-] BiHRNet: A Binary high-resolution network for Human Pose Estimation [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2311.10296">paper</a>]</p>
<h3 id="3D-Human-Pose-Estimation">3D Human Pose Estimation</h3>
<p>[<strong>AAAI 2024</strong>] Lifting by Image - Leveraging Image Cues for Accurate 3D Human Pose Estimation [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2312.15636">paper</a>]</p>
<h3 id="3D-Human-Motion-Prediction">3D Human Motion Prediction</h3>
<p>[<strong>KBS 2024</strong>] April-GCN: Adjacency Position-velocity Relationship Interaction Learning GCN for Human motion prediction [<a target="_blank" rel="noopener" href="https://authors.elsevier.com/sd/article/S0950-7051(24)00248-X">paper</a>]</p>
<p>[<strong>TNNLS 2023</strong>] Learning Constrained Dynamic Correlations in Spatiotemporal Graphs for Motion Prediction [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/10138910">paper</a>] [<a target="_blank" rel="noopener" href="https://github.com/Jaakk0F/DSTD-GCN">code</a>]</p>
<p>[<strong>TCSVT 2023</strong>] Collaborative Multi-Dynamic Pattern Modeling for Human Motion Prediction [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/10025861">paper</a>]</p>
<p>[<strong>TCSVT 2022</strong>] Towards more realistic human motion prediction with attention to motion coordination [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9745623/">paper</a>]</p>
<p>[<strong>TCSVT 2021</strong>] TrajectoryCNN: a new spatio-temporal feature learning network for human motion prediction [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/9186039">paper</a>] [<a target="_blank" rel="noopener" href="https://github.com/lily2lab/TrajectoryCNN">code</a>]</p>
<p>[<strong>Neurocomputing 2024</strong>] Physics-constrained Attack against Convolution-based Human Motion Prediction [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2306.11990">paper</a>] [<a target="_blank" rel="noopener" href="https://github.com/ChengxuDuan/advHMP">code</a>]</p>
<p>[<strong>Neurocomputing 2022</strong>] Temporal consistency two-stream CNN for human motion prediction [<a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/abs/pii/S0925231221014892?via%3Dihub">paper</a>]</p>
<p>[<strong>机器人 2022</strong>] 面向人体动作预测的对称残差网络 [<a target="_blank" rel="noopener" href="https://robot.sia.cn/cn/article/doi/10.13973/j.cnki.robot.210188#:~:text=%E6%91%98%E8%A6%81%3A%20%E4%B8%BA%E4%BA%86%E7%A0%94%E7%A9%B6%E4%B8%8D%E5%90%8C%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5%E6%96%B9%E5%BC%8F%E5%AF%B9%E4%BA%BA%E4%BD%93%E5%8A%A8%E4%BD%9C%E9%A2%84%E6%B5%8B%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%BD%B1%E5%93%8D%EF%BC%8C%E6%8E%A2%E8%AE%A8%E4%BA%86%E5%9C%A8%E4%BF%9D%E6%8C%81%E7%BD%91%E7%BB%9C%E6%B7%B1%E5%BA%A6%E4%B8%80%E5%AE%9A%E7%9A%84%E6%83%85%E5%86%B5%E4%B8%8B%EF%BC%8C%E5%A6%82%E4%BD%95%E5%88%A9%E7%94%A8%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5%E6%9E%84%E6%88%90%E4%B8%80%E4%B8%AA%E9%AB%98%E6%95%88%E6%8D%95%E6%8D%89%E4%BA%BA%E4%BD%93%E5%8A%A8%E4%BD%9C%E7%89%B9%E5%BE%81%E7%9A%84%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B%E3%80%82%20%E9%80%9A%E8%BF%87%E8%A7%82%E5%AF%9F%E4%BA%BA%E4%BD%93%E9%AA%A8%E9%AA%BC%E5%85%B3%E8%8A%82%E7%82%B9%E6%8E%92%E5%88%97%E6%96%B9%E5%BC%8F%EF%BC%8C%E6%8F%90%E5%87%BA%E4%B8%80%E7%A7%8D%E9%80%82%E7%94%A8%E4%BA%8E%E4%BA%BA%E4%BD%93%E9%AA%A8%E9%AA%BC%E5%85%B3%E8%8A%82%E7%82%B9%E9%A2%84%E6%B5%8B%E7%9A%84%E5%AF%B9%E7%A7%B0%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5%E6%96%B9%E6%B3%95%EF%BC%8C%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%AF%A5%E6%96%B9%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%BA%86%E5%AF%B9%E7%A7%B0%E6%AE%8B%E5%B7%AE%E5%9D%97,%28symmetric%20residual%20block%EF%BC%8CSRB%29%E3%80%82">paper</a>]</p>
<p>[<strong>MPE 2020</strong>] A Hierarchical Static-Dynamic Encoder-Decoder Structure for 3D Human Motion Prediction with Residual CNNs [<a target="_blank" rel="noopener" href="https://www.hindawi.com/journals/mpe/2020/7064910/">paper</a>] [<a target="_blank" rel="noopener" href="https://github.com/liujin0/SDnet">code</a>]</p>
<p>[<strong>Cognitive Computation and Systems 2020</strong>] Stacked residual blocks based encoder–decoder framework for human motion prediction[<a target="_blank" rel="noopener" href="https://github.com/lily2lab/residual_prediction_network">code</a>]</p>
<p>[-]Uncertainty-aware Human Motion Prediction [<a target="_blank" rel="noopener" href="https://scholar.google.com/scholar?oi=bibs&amp;cluster=11543079145147482533&amp;btnI=1&amp;hl=en">paper</a>]</p>
<p>[-] MSSL: Multi-scale Semi-decoupled Spatiotemporal Learning for 3D human motion prediction [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2010.05133">paper</a>][<a target="_blank" rel="noopener" href="https://github.com/lily2lab/MSSL">code</a>]</p>
<p>[-] DeepSSM: Deep State-Space Model for 3D Human Motion Prediction [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2005.12155">paper</a>] [<a target="_blank" rel="noopener" href="https://github.com/lily2lab/DeepSSM">code</a>]</p>
<h3 id="Early-Action-Prediction">Early Action Prediction</h3>
<p>[<strong>TIP 2024</strong>] Rich Action-semantic Consistent Knowledge for Early Action Prediction [<a target="_blank" rel="noopener" href="https://www.semanticscholar.org/reader/7ec7b4929c73ade2c926b65e88bdefaa03148115">paper</a>] [<a target="_blank" rel="noopener" href="https://github.com/lily2lab/RACK">code</a>]</p>
<p>[<strong>ICCSIP 2022</strong>] A discussion of data sampling strategies for early action prediction [<a target="_blank" rel="noopener" href="https://link.springer.com/chapter/10.1007/978-981-16-9247-5_24">paper</a>]</p>
<p>[<strong>中国自动化大会 2023</strong>] An end-to-end multi-scale network for action prediction in videos</p>
<h3 id="Skeleton-based-Human-Action-Recognition">Skeleton-based Human Action Recognition</h3>
<p>[<strong>TCSVT 2024</strong>] SiT-MLP: A Simple MLP with Point-wise Topology Feature Learning for Skeleton-based Action Recognition [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/10495051">paper</a>] [<a target="_blank" rel="noopener" href="https://github.com/BUPTSJZhang/SiT-MLP">code</a>]</p>
<p>[<strong>RAS 2020</strong>] DWnet: Deep-wide network for 3D action recognition [<a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/abs/pii/S0921889019308176">paper</a>] [<a target="_blank" rel="noopener" href="https://github.com/YHDang/DWnet">code</a>]</p>
<p>[-] Spatial-Temporal Decoupling Contrastive Learning for Skeleton-based Human Action Recognition [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2312.15144">paper</a>] [<a target="_blank" rel="noopener" href="https://github.com/BUPTSJZhang/STD-CL">code</a>]</p>
<h3 id="Group-Activity-Recognition">Group Activity Recognition</h3>
<p>[<strong>KBS 2024</strong>] MLP-AIR: An effective MLP-based module for actor interaction relation learning in group activity recognition [<a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S0950705124010876">paper</a>]</p>
<h3 id="Uncertainty-aware-Scene-Understanding-with-Point-Clouds">Uncertainty-aware Scene Understanding with Point Clouds</h3>
<p>[<strong>TGRS 2023</strong>] Neighborhood Spatial Aggregation MC Dropout for Efficient Uncertainty-aware Semantic Segmentation in Point Clouds [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/10247069/">paper</a>] [<a target="_blank" rel="noopener" href="https://github.com/chaoqi7/Uncertainty_Estimation_PCSS">code</a>]</p>
<p>[<strong>TCSVT 2023</strong>] Instance-incremental Scene Graph Generation from Real-world Point Clouds via Normalizing Flows [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/10164228/">paper</a>] [<a target="_blank" rel="noopener" href="https://github.com/chaoqi7/GPL3D">code</a>]</p>
<p>[<strong>TIM 2020</strong>] Multigranularity Semantic Labeling of Point Clouds for the Measurement of the Rail Tanker Component With Structure Modeling [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/9207911/">paper</a>] [<a target="_blank" rel="noopener" href="https://github.com/chaoqi7/Multi-granularity-Semantic-Labeling-with-Structure-Modeling-TIM">code</a>]</p>
<p>[<strong>ICRA 2021</strong>] Neighborhood Spatial Aggregation based Efficient Uncertainty Estimation for Point Cloud Semantic Segmentation  [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/9560972/">paper</a>] [<a target="_blank" rel="noopener" href="https://github.com/chaoqi7/Uncertainty_Estimation_PCSS">code</a>]</p>
<p>[<strong>Tsinghua Science and Technology 2023</strong>] Dynamic Scene Graph Generation of Point Clouds with Structural Representation Learning [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/10225283/">paper</a>]</p>
<h3 id="Audio-Visual-Learning">Audio Visual Learning</h3>
<p>[<strong>TMM 2023</strong>] Leveraging the Video-level Semantic Consistency of Event for Audio-visual Event Localization [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/10286391">paper</a>] [<a target="_blank" rel="noopener" href="https://github.com/Bravo5542/VSCG">code</a>]</p>
<p>[<strong>EMNLP 2023</strong>] Target-Aware Spatio-Temporal Reasoning via Answering Questions in Dynamic Audio-Visual Scenarios [<a target="_blank" rel="noopener" href="https://aclanthology.org/2023.findings-emnlp.630/">paper</a>] [<a target="_blank" rel="noopener" href="https://github.com/Bravo5542/TJSTG">code</a>]</p>
<p>[<strong>计算机应用 2021</strong>] 基于关键帧筛选网络的视听联合动作识别 [<a target="_blank" rel="noopener" href="http://www.joca.cn/CN/10.11772/j.issn.1001-9081.2021060995">paper</a>]</p>
<p>[-] Past Future Motion Guided Network for Audio Visual Event Localization [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2205.03802v1">paper</a>]</p>
<h3 id="Audio-and-Speech-Processing">Audio and Speech Processing</h3>
<p>[<strong>ICPR 2024</strong>] Full-frequency dynamic convolution: a physical frequency-dependent convolution for sound event detection [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2401.04976">paper</a>] [<a target="_blank" rel="noopener" href="https://github.com/Harper812/FFDConv">code</a>]</p>
<p>[<strong>Interspeech 2024</strong>]  MFF-EINV2: Multi-scale Feature Fusion across Spectral-Spatial-Temporal Domains for Sound Event Localization and Detection [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2406.08771">paper</a>] [<a target="_blank" rel="noopener" href="https://github.com/muuda/MFF-EINV2">code</a>]</p>
<h3 id="Medical-Image-Segmentation">Medical Image Segmentation</h3>
<p>[<strong>IEEE-CYBER 2023</strong>] Multi-task Learning Network for CT Whole Heart Segmentation [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/10256432">paper</a>]</p>
<p>[<strong>Biomedical Signal Processing and Control 2022</strong>] DC-net: Dual-Consistency Semi-Supervised Learning for 3D Left Atrium Segmentation from MRI [<a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/abs/pii/S1746809422003858">paper</a>]</p>
<h3 id="Adversarial-Attack">Adversarial Attack</h3>
<p>[<strong>Neurocomputing 2023</strong>] Physics-constrained attack against convolution-based human motion prediction [<a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S0925231224000432?via%3Dihub">paper</a>]</p>
<h3 id="Others">Others</h3>
<p>[<strong>MTAP2023</strong>] Transfer the global knowledge for current gaze estimation [<a target="_blank" rel="noopener" href="https://link.springer.com/article/10.1007/s11042-023-17484-2">paper</a>]</p>
<p>[<strong>TCSVT 2021</strong>] Energy-based Periodicity Mining with Deep Features for Action Repetition Counting in Unconstrained Videos [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/9339959">paper</a>] [<a target="_blank" rel="noopener" href="https://github.com/BUPT-COST-lab/ActionCounting">code</a>]</p>
<p>[<strong>ROBIO 2019</strong>]DBNet: A New Generalized Structure Efficient for Classification [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/8961680/">paper</a>] [<a target="_blank" rel="noopener" href="https://github.com/YHDang/DBNet">code</a>]</p>
<p>[-] SDVRF: Sparse-to-Dense Voxel Region Fusion for Multi-modal 3D Object Detection [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2304.08304">paper</a>]</p>
<h2 id="Last-update-August-22-2024">Last update: August 22, 2024</h2>
<p>Feel free to contact us at <a href="mailto:7858833@bupt.edu.cn">7858833@bupt.edu.cn</a>, or <a href="mailto:zsj@bupt.edu.cn">zsj@bupt.edu.cn</a>.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/10/21/%E6%B5%8B%E8%AF%95%E7%AC%AC2%E7%AF%87%E6%96%87%E7%AB%A0/" data-id="cm2j2762e000170p6gg9001ww" data-title="测试第2篇文章" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-测试第一篇文章" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/10/21/%E6%B5%8B%E8%AF%95%E7%AC%AC%E4%B8%80%E7%AF%87%E6%96%87%E7%AB%A0/" class="article-date">
  <time class="dt-published" datetime="2024-10-21T11:44:56.000Z" itemprop="datePublished">2024-10-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/10/21/%E6%B5%8B%E8%AF%95%E7%AC%AC%E4%B8%80%E7%AF%87%E6%96%87%E7%AB%A0/">测试第一篇文章</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/10/21/%E6%B5%8B%E8%AF%95%E7%AC%AC%E4%B8%80%E7%AF%87%E6%96%87%E7%AB%A0/" data-id="cm2j2762g000370p6exgqc4bp" data-title="测试第一篇文章" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-hello-world" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/10/21/hello-world/" class="article-date">
  <time class="dt-published" datetime="2024-10-21T11:28:35.431Z" itemprop="datePublished">2024-10-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/10/21/hello-world/">Hello World</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start">Quick Start</h2>
<h3 id="Create-a-new-post">Create a new post</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server">Run server</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/10/21/hello-world/" data-id="cm2j2762b000070p6dgxa62pr" data-title="Hello World" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/10/">October 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/10/21/yolo-v9%E8%BF%90%E8%A1%8C%E5%AE%9E%E5%BD%95/">yolo v9运行实录</a>
          </li>
        
          <li>
            <a href="/2024/10/21/%E6%B5%8B%E8%AF%95%E7%AC%AC2%E7%AF%87%E6%96%87%E7%AB%A0/">测试第2篇文章</a>
          </li>
        
          <li>
            <a href="/2024/10/21/%E6%B5%8B%E8%AF%95%E7%AC%AC%E4%B8%80%E7%AF%87%E6%96%87%E7%AB%A0/">测试第一篇文章</a>
          </li>
        
          <li>
            <a href="/2024/10/21/hello-world/">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>