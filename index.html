<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-测试第一篇文章" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/10/21/%E6%B5%8B%E8%AF%95%E7%AC%AC%E4%B8%80%E7%AF%87%E6%96%87%E7%AB%A0/" class="article-date">
  <time class="dt-published" datetime="2024-10-21T11:44:56.000Z" itemprop="datePublished">2024-10-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/10/21/%E6%B5%8B%E8%AF%95%E7%AC%AC%E4%B8%80%E7%AF%87%E6%96%87%E7%AB%A0/">测试第一篇文章</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/10/21/%E6%B5%8B%E8%AF%95%E7%AC%AC%E4%B8%80%E7%AF%87%E6%96%87%E7%AB%A0/" data-id="cm2iyk7f800016kp6cnifbvki" data-title="测试第一篇文章" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-hello-world" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/10/21/hello-world/" class="article-date">
  <time class="dt-published" datetime="2024-10-21T11:28:35.431Z" itemprop="datePublished">2024-10-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/10/21/hello-world/">Hello World</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start">Quick Start</h2>
<h3 id="Create-a-new-post">Create a new post</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server">Run server</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/10/21/hello-world/" data-id="cm2iyk7f500006kp68vy9de0e" data-title="Hello World" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-test" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/10/21/test/" class="article-date">
  <time class="dt-published" datetime="2024-10-21T08:45:23.078Z" itemprop="datePublished">2024-10-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/10/21/test/">ICOST</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="Hi-👋-Here-is-the-homepage-of-Intelligent-Cognitive-Systems-Laboratory-iCOST-Beijing-University-of-Posts-and-Telecommunications">Hi 👋. Here is the homepage of Intelligent Cognitive Systems Laboratory (iCOST), Beijing University of Posts and Telecommunications.</h2>
<!--

**Here are some ideas to get you started:**

🙋‍♀️ A short introduction - what is your organization all about?
🌈 Contribution guidelines - how can the community get involved?
👩‍💻 Useful resources - where can the community find your docs? Is there anything else the community should know?
🍿 Fun facts - what does your team eat for breakfast?
🧙 Remember, you can do mighty things with the power of [Markdown](https://docs.github.com/github/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax)
-->
<h2 id="Table-of-Contents">Table of Contents</h2>
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#papers">Papers</a>
<ul>
<li><a href="#2d-human-pose-estimation">2D Human Pose Estimation</a></li>
<li><a href="#3d-human-pose-estimation">3D Human Pose Estimation</a></li>
<li><a href="#3d-human-motion-prediction">3D Human Motion Prediction</a></li>
<li><a href="#early-action-prediction">Early Action Prediction</a></li>
<li><a href="#skeleton-based-human-action-recognition">Skeleton-based Human Action Recognition</a></li>
<li><a href="#group-activity-recognition">Group Activity Recognition</a></li>
<li><a href="#uncertainty-aware-scene-understanding-with-point-clouds">Uncertainty-aware Scene Understanding with Point Clouds</a></li>
<li><a href="#audio-visual-learning">Audio-Visual Learning</a></li>
<li><a href="#audio-and-speech-processing">Audio and Speech Processing</a></li>
<li><a href="#medical-image-segmentation">Medical Image Segmentation</a></li>
<li><a href="#adversarial-attack">Adversarial Attack</a></li>
<li><a href="#others">Others</a></li>
</ul>
</li>
</ul>
<h2 id="Introduction">Introduction</h2>
<p>The Intelligent Cognitive Systems Laboratory (iCost) at BUPT (Beijing University of Posts and Telecommunications) is actively engaged in long-term research in multiple cutting-edge fields, including computer vision and embodied intelligence. Our research spans various sub-domains such as action recognition, human pose prediction and estimation, uncertainty research, multimodal and audio-visual learning, audio-visual event detection, medical image segmentation, 3D object detection, adversarial strategies, embodied navigation, and robot grasping.</p>
<p>Our research team has achieved substantial results, publishing numerous high-quality research papers in internationally recognized and authoritative journals such as IEEE Transactions on Image Processing (IEEE TIP), IEEE Transactions on Circuits and Systems for Video Technology (IEEE TCSVT), and top-tier conferences like AAAI.</p>
<p>In the Intelligent Cognitive Systems Laboratory, we encourage communication and collaboration among team members, fostering a rigorous and harmonious academic atmosphere. We warmly welcome scholars, researchers, and students who are passionate about artificial intelligence and related fields to join us or collaborate. We believe that everyone with a curiosity for science can find their stage here.</p>
<h2 id="Papers">Papers</h2>
<h3 id="2D-Human-Pose-Estimation">2D Human Pose Estimation</h3>
<p>[<strong>PR 2024</strong>] Kinematics Modeling Network for Video-based Human Pose Estimation [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2207.10971.pdf">paper</a>] [<a target="_blank" rel="noopener" href="https://github.com/YHDang/KIMNet">code</a>]</p>
<p>[<strong>TIP 2022</strong>] Relation-Based Associative Joint Location for Human Pose Estimation in Videos [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/9786543">paper</a>] [<a target="_blank" rel="noopener" href="https://github.com/YHDang/pose-estimation">code</a>]</p>
<p>[<strong>KBS 2024</strong>] DHRNet: A Dual-Path Hierarchical Relation Network for Multi-Person Pose Estimation [<a target="_blank" rel="noopener" href="https://github.com/YHDang/DHRNet">code</a>]</p>
<p>[-] BiHRNet: A Binary high-resolution network for Human Pose Estimation [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2311.10296">paper</a>]</p>
<h3 id="3D-Human-Pose-Estimation">3D Human Pose Estimation</h3>
<p>[<strong>AAAI 2024</strong>] Lifting by Image - Leveraging Image Cues for Accurate 3D Human Pose Estimation [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2312.15636">paper</a>]</p>
<h3 id="3D-Human-Motion-Prediction">3D Human Motion Prediction</h3>
<p>[<strong>KBS 2024</strong>] April-GCN: Adjacency Position-velocity Relationship Interaction Learning GCN for Human motion prediction [<a target="_blank" rel="noopener" href="https://authors.elsevier.com/sd/article/S0950-7051(24)00248-X">paper</a>]</p>
<p>[<strong>TNNLS 2023</strong>] Learning Constrained Dynamic Correlations in Spatiotemporal Graphs for Motion Prediction [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/10138910">paper</a>] [<a target="_blank" rel="noopener" href="https://github.com/Jaakk0F/DSTD-GCN">code</a>]</p>
<p>[<strong>TCSVT 2023</strong>] Collaborative Multi-Dynamic Pattern Modeling for Human Motion Prediction [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/10025861">paper</a>]</p>
<p>[<strong>TCSVT 2022</strong>] Towards more realistic human motion prediction with attention to motion coordination [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9745623/">paper</a>]</p>
<p>[<strong>TCSVT 2021</strong>] TrajectoryCNN: a new spatio-temporal feature learning network for human motion prediction [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/9186039">paper</a>] [<a target="_blank" rel="noopener" href="https://github.com/lily2lab/TrajectoryCNN">code</a>]</p>
<p>[<strong>Neurocomputing 2024</strong>] Physics-constrained Attack against Convolution-based Human Motion Prediction [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2306.11990">paper</a>] [<a target="_blank" rel="noopener" href="https://github.com/ChengxuDuan/advHMP">code</a>]</p>
<p>[<strong>Neurocomputing 2022</strong>] Temporal consistency two-stream CNN for human motion prediction [<a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/abs/pii/S0925231221014892?via%3Dihub">paper</a>]</p>
<p>[<strong>机器人 2022</strong>] 面向人体动作预测的对称残差网络 [<a target="_blank" rel="noopener" href="https://robot.sia.cn/cn/article/doi/10.13973/j.cnki.robot.210188#:~:text=%E6%91%98%E8%A6%81%3A%20%E4%B8%BA%E4%BA%86%E7%A0%94%E7%A9%B6%E4%B8%8D%E5%90%8C%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5%E6%96%B9%E5%BC%8F%E5%AF%B9%E4%BA%BA%E4%BD%93%E5%8A%A8%E4%BD%9C%E9%A2%84%E6%B5%8B%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%BD%B1%E5%93%8D%EF%BC%8C%E6%8E%A2%E8%AE%A8%E4%BA%86%E5%9C%A8%E4%BF%9D%E6%8C%81%E7%BD%91%E7%BB%9C%E6%B7%B1%E5%BA%A6%E4%B8%80%E5%AE%9A%E7%9A%84%E6%83%85%E5%86%B5%E4%B8%8B%EF%BC%8C%E5%A6%82%E4%BD%95%E5%88%A9%E7%94%A8%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5%E6%9E%84%E6%88%90%E4%B8%80%E4%B8%AA%E9%AB%98%E6%95%88%E6%8D%95%E6%8D%89%E4%BA%BA%E4%BD%93%E5%8A%A8%E4%BD%9C%E7%89%B9%E5%BE%81%E7%9A%84%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B%E3%80%82%20%E9%80%9A%E8%BF%87%E8%A7%82%E5%AF%9F%E4%BA%BA%E4%BD%93%E9%AA%A8%E9%AA%BC%E5%85%B3%E8%8A%82%E7%82%B9%E6%8E%92%E5%88%97%E6%96%B9%E5%BC%8F%EF%BC%8C%E6%8F%90%E5%87%BA%E4%B8%80%E7%A7%8D%E9%80%82%E7%94%A8%E4%BA%8E%E4%BA%BA%E4%BD%93%E9%AA%A8%E9%AA%BC%E5%85%B3%E8%8A%82%E7%82%B9%E9%A2%84%E6%B5%8B%E7%9A%84%E5%AF%B9%E7%A7%B0%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5%E6%96%B9%E6%B3%95%EF%BC%8C%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%AF%A5%E6%96%B9%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%BA%86%E5%AF%B9%E7%A7%B0%E6%AE%8B%E5%B7%AE%E5%9D%97,%28symmetric%20residual%20block%EF%BC%8CSRB%29%E3%80%82">paper</a>]</p>
<p>[<strong>MPE 2020</strong>] A Hierarchical Static-Dynamic Encoder-Decoder Structure for 3D Human Motion Prediction with Residual CNNs [<a target="_blank" rel="noopener" href="https://www.hindawi.com/journals/mpe/2020/7064910/">paper</a>] [<a target="_blank" rel="noopener" href="https://github.com/liujin0/SDnet">code</a>]</p>
<p>[<strong>Cognitive Computation and Systems 2020</strong>] Stacked residual blocks based encoder–decoder framework for human motion prediction[<a target="_blank" rel="noopener" href="https://github.com/lily2lab/residual_prediction_network">code</a>]</p>
<p>[-]Uncertainty-aware Human Motion Prediction [<a target="_blank" rel="noopener" href="https://scholar.google.com/scholar?oi=bibs&amp;cluster=11543079145147482533&amp;btnI=1&amp;hl=en">paper</a>]</p>
<p>[-] MSSL: Multi-scale Semi-decoupled Spatiotemporal Learning for 3D human motion prediction [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2010.05133">paper</a>][<a target="_blank" rel="noopener" href="https://github.com/lily2lab/MSSL">code</a>]</p>
<p>[-] DeepSSM: Deep State-Space Model for 3D Human Motion Prediction [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2005.12155">paper</a>] [<a target="_blank" rel="noopener" href="https://github.com/lily2lab/DeepSSM">code</a>]</p>
<h3 id="Early-Action-Prediction">Early Action Prediction</h3>
<p>[<strong>TIP 2024</strong>] Rich Action-semantic Consistent Knowledge for Early Action Prediction [<a target="_blank" rel="noopener" href="https://www.semanticscholar.org/reader/7ec7b4929c73ade2c926b65e88bdefaa03148115">paper</a>] [<a target="_blank" rel="noopener" href="https://github.com/lily2lab/RACK">code</a>]</p>
<p>[<strong>ICCSIP 2022</strong>] A discussion of data sampling strategies for early action prediction [<a target="_blank" rel="noopener" href="https://link.springer.com/chapter/10.1007/978-981-16-9247-5_24">paper</a>]</p>
<p>[<strong>中国自动化大会 2023</strong>] An end-to-end multi-scale network for action prediction in videos</p>
<h3 id="Skeleton-based-Human-Action-Recognition">Skeleton-based Human Action Recognition</h3>
<p>[<strong>TCSVT 2024</strong>] SiT-MLP: A Simple MLP with Point-wise Topology Feature Learning for Skeleton-based Action Recognition [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/10495051">paper</a>] [<a target="_blank" rel="noopener" href="https://github.com/BUPTSJZhang/SiT-MLP">code</a>]</p>
<p>[<strong>RAS 2020</strong>] DWnet: Deep-wide network for 3D action recognition [<a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/abs/pii/S0921889019308176">paper</a>] [<a target="_blank" rel="noopener" href="https://github.com/YHDang/DWnet">code</a>]</p>
<p>[-] Spatial-Temporal Decoupling Contrastive Learning for Skeleton-based Human Action Recognition [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2312.15144">paper</a>] [<a target="_blank" rel="noopener" href="https://github.com/BUPTSJZhang/STD-CL">code</a>]</p>
<h3 id="Group-Activity-Recognition">Group Activity Recognition</h3>
<p>[<strong>KBS 2024</strong>] MLP-AIR: An effective MLP-based module for actor interaction relation learning in group activity recognition [<a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S0950705124010876">paper</a>]</p>
<h3 id="Uncertainty-aware-Scene-Understanding-with-Point-Clouds">Uncertainty-aware Scene Understanding with Point Clouds</h3>
<p>[<strong>TGRS 2023</strong>] Neighborhood Spatial Aggregation MC Dropout for Efficient Uncertainty-aware Semantic Segmentation in Point Clouds [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/10247069/">paper</a>] [<a target="_blank" rel="noopener" href="https://github.com/chaoqi7/Uncertainty_Estimation_PCSS">code</a>]</p>
<p>[<strong>TCSVT 2023</strong>] Instance-incremental Scene Graph Generation from Real-world Point Clouds via Normalizing Flows [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/10164228/">paper</a>] [<a target="_blank" rel="noopener" href="https://github.com/chaoqi7/GPL3D">code</a>]</p>
<p>[<strong>TIM 2020</strong>] Multigranularity Semantic Labeling of Point Clouds for the Measurement of the Rail Tanker Component With Structure Modeling [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/9207911/">paper</a>] [<a target="_blank" rel="noopener" href="https://github.com/chaoqi7/Multi-granularity-Semantic-Labeling-with-Structure-Modeling-TIM">code</a>]</p>
<p>[<strong>ICRA 2021</strong>] Neighborhood Spatial Aggregation based Efficient Uncertainty Estimation for Point Cloud Semantic Segmentation  [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/9560972/">paper</a>] [<a target="_blank" rel="noopener" href="https://github.com/chaoqi7/Uncertainty_Estimation_PCSS">code</a>]</p>
<p>[<strong>Tsinghua Science and Technology 2023</strong>] Dynamic Scene Graph Generation of Point Clouds with Structural Representation Learning [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/10225283/">paper</a>]</p>
<h3 id="Audio-Visual-Learning">Audio Visual Learning</h3>
<p>[<strong>TMM 2023</strong>] Leveraging the Video-level Semantic Consistency of Event for Audio-visual Event Localization [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/10286391">paper</a>] [<a target="_blank" rel="noopener" href="https://github.com/Bravo5542/VSCG">code</a>]</p>
<p>[<strong>EMNLP 2023</strong>] Target-Aware Spatio-Temporal Reasoning via Answering Questions in Dynamic Audio-Visual Scenarios [<a target="_blank" rel="noopener" href="https://aclanthology.org/2023.findings-emnlp.630/">paper</a>] [<a target="_blank" rel="noopener" href="https://github.com/Bravo5542/TJSTG">code</a>]</p>
<p>[<strong>计算机应用 2021</strong>] 基于关键帧筛选网络的视听联合动作识别 [<a target="_blank" rel="noopener" href="http://www.joca.cn/CN/10.11772/j.issn.1001-9081.2021060995">paper</a>]</p>
<p>[-] Past Future Motion Guided Network for Audio Visual Event Localization [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2205.03802v1">paper</a>]</p>
<h3 id="Audio-and-Speech-Processing">Audio and Speech Processing</h3>
<p>[<strong>ICPR 2024</strong>] Full-frequency dynamic convolution: a physical frequency-dependent convolution for sound event detection [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2401.04976">paper</a>] [<a target="_blank" rel="noopener" href="https://github.com/Harper812/FFDConv">code</a>]</p>
<p>[<strong>Interspeech 2024</strong>]  MFF-EINV2: Multi-scale Feature Fusion across Spectral-Spatial-Temporal Domains for Sound Event Localization and Detection [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2406.08771">paper</a>] [<a target="_blank" rel="noopener" href="https://github.com/muuda/MFF-EINV2">code</a>]</p>
<h3 id="Medical-Image-Segmentation">Medical Image Segmentation</h3>
<p>[<strong>IEEE-CYBER 2023</strong>] Multi-task Learning Network for CT Whole Heart Segmentation [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/10256432">paper</a>]</p>
<p>[<strong>Biomedical Signal Processing and Control 2022</strong>] DC-net: Dual-Consistency Semi-Supervised Learning for 3D Left Atrium Segmentation from MRI [<a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/abs/pii/S1746809422003858">paper</a>]</p>
<h3 id="Adversarial-Attack">Adversarial Attack</h3>
<p>[<strong>Neurocomputing 2023</strong>] Physics-constrained attack against convolution-based human motion prediction [<a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S0925231224000432?via%3Dihub">paper</a>]</p>
<h3 id="Others">Others</h3>
<p>[<strong>MTAP2023</strong>] Transfer the global knowledge for current gaze estimation [<a target="_blank" rel="noopener" href="https://link.springer.com/article/10.1007/s11042-023-17484-2">paper</a>]</p>
<p>[<strong>TCSVT 2021</strong>] Energy-based Periodicity Mining with Deep Features for Action Repetition Counting in Unconstrained Videos [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/9339959">paper</a>] [<a target="_blank" rel="noopener" href="https://github.com/BUPT-COST-lab/ActionCounting">code</a>]</p>
<p>[<strong>ROBIO 2019</strong>]DBNet: A New Generalized Structure Efficient for Classification [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/8961680/">paper</a>] [<a target="_blank" rel="noopener" href="https://github.com/YHDang/DBNet">code</a>]</p>
<p>[-] SDVRF: Sparse-to-Dense Voxel Region Fusion for Multi-modal 3D Object Detection [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2304.08304">paper</a>]</p>
<h2 id="Last-update-August-22-2024">Last update: August 22, 2024</h2>
<p>Feel free to contact us at <a href="mailto:7858833@bupt.edu.cn">7858833@bupt.edu.cn</a>, or <a href="mailto:zsj@bupt.edu.cn">zsj@bupt.edu.cn</a>.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/10/21/test/" data-id="cm2iyk7f900026kp63qx0ciw0" data-title="ICOST" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/10/">October 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/10/21/%E6%B5%8B%E8%AF%95%E7%AC%AC%E4%B8%80%E7%AF%87%E6%96%87%E7%AB%A0/">测试第一篇文章</a>
          </li>
        
          <li>
            <a href="/2024/10/21/hello-world/">Hello World</a>
          </li>
        
          <li>
            <a href="/2024/10/21/test/">ICOST</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>